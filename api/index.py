# -*- coding: utf-8 -*-
"""
Vercel API endpoint for Text Site AI Extractor
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import requests
from bs4 import BeautifulSoup
import re
import difflib
from googlesearch import search

app = Flask(__name__)
CORS(app)

def remove_duplicate_content(text: str) -> str:
    """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙƒØ±Ø± ÙˆØ§Ù„Ù…ØªÙƒØ±Ø± Ù…Ù† Ø§Ù„Ù†Øµ"""
    if not text:
        return ""
    
    paragraphs = text.split('\n\n')
    paragraphs = [p.strip() for p in paragraphs if p.strip()]
    
    unique_paragraphs = []
    seen_paragraphs = set()
    
    for paragraph in paragraphs:
        if len(paragraph) < 50:
            continue
            
        clean_paragraph = re.sub(r'\s+', ' ', paragraph.strip())
        
        is_duplicate = False
        for seen in seen_paragraphs:
            similarity = difflib.SequenceMatcher(None, clean_paragraph, seen).ratio()
            if similarity > 0.7:
                is_duplicate = True
                break
        
        if not is_duplicate:
            unique_paragraphs.append(paragraph)
            seen_paragraphs.add(clean_paragraph)
    
    return '\n\n'.join(unique_paragraphs)

def filter_content_quality(text: str) -> str:
    """ÙÙ„ØªØ±Ø© Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„Ù†ØµÙˆØµ ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©"""
    if not text:
        return ""
    
    unwanted_patterns = [
        r'Ù‡Ù„ Ø§Ø³ØªÙØ¯Øª Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„ØµÙØ­Ø©ØŸ',
        r'Ù†Ø¹Ù…Ù„Ø§',
        r'Ø´ÙƒØ±Ø§ Ù„Ù…Ø´Ø§Ø±ÙƒØªÙƒ',
        r'Ø£Ø¶Ù Ø§Ù„Ø³Ø¨Ø¨',
        r'Ù‡Ø°Ø§ Ø§Ù„Ø­Ù‚Ù„ Ù…Ø·Ù„ÙˆØ¨',
        r'Ø§Ù„Ø±Ø¬Ø§Ø¡ ÙƒØªØ§Ø¨Ø© Ù†Øµ ØµØ­ÙŠØ­',
        r'Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚',
        r'Ø¥Ù„ØºØ§Ø¡',
        r'Ø´Ø§Ø±Ùƒ Ø§Ù„Ù…Ù‚Ø§Ù„Ø©',
        r'ÙÙŠØ³Ø¨ÙˆÙƒØªÙˆÙŠØªØ±',
        r'Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø°Ø§Øª ØµÙ„Ø©',
        r'Ø§Ù†Ø¶Ù… Ø§Ù„ÙŠÙ†Ø§',
        r'Tags:',
        r'Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø§Øª',
        r'Ø§Ø¹Ø±Ù Ø§Ù„Ù…Ø²ÙŠØ¯',
        r'Ø¯ÙˆØ±Ø§Øª Ù‚Ø¯ ØªÙ‡Ù…Ùƒ',
        r'Ù…Ø³Ø§Ø±Ø§Øª ØªØ¹Ù„ÙŠÙ…ÙŠØ©',
        r'ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª',
        r'Ù…Ù‚Ø§Ù„Ø§Øª Ø´Ø§Ø¦Ø¹Ø©',
        r'ÙƒØªØ§Ø¨Ø© :',
        r'Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«:',
        r'ØªØ­Ù…ÙŠÙ„',
        r'Ù…Ø´Ø§Ù‡Ø¯Ø©',
        r'Ù„Ù„Ù…Ø²ÙŠØ¯ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰',
        r'ØªÙ…Øª Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø¨ÙˆØ§Ø³Ø·Ø©:',
        r'Ù…Ø­ØªÙˆÙŠØ§Øª',
        r'Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹',
        r'Ù…Ø¬Ù„ÙˆØ¨Ø© Ù…Ù†',
        r'Ù‡Ù„ ÙƒØ§Ù† Ø§Ù„Ù…Ù‚Ø§Ù„ Ù…ÙÙŠØ¯Ø§Ù‹ØŸ',
        r'Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¬ØªÙŠØ§Ø² Ø§Ù„Ø¥Ø®ØªØ¨Ø§Ø±',
        r'Ø±Ø§Ø¦Ø¹!',
        r'Ù‡Ù„ Ù„Ø¯ÙŠÙƒ Ù…Ù‚ØªØ±Ø­Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨ØªÙƒ Ø£ÙƒØ«Ø±ØŸ',
        r'Ø¨Ø±ÙŠØ¯Ùƒ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ*',
        r'Ø§Ù„Ø¥Ø³Ù…*',
        r'Ø§Ù„Ø¥Ø´ØªØ±Ø§Ùƒ Ø¨Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø±ÙŠØ¯ Ù…ÙˆØ¶ÙˆØ¹',
        r'Ù†Ø£Ø³Ù Ù„Ø°Ù„Ùƒ!',
        r'Ù„Ù…Ø§Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù‚Ø§Ù„ ØºÙŠØ± Ù…ÙÙŠØ¯ØŸ',
        r'Ù…Ø·Ù„ÙˆØ¨ Ø§Ù„Ø¥Ø®ØªÙŠØ§Ø±',
        r'ÙŠØ¬Ø¨ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ù„Ù„Ù…ØªØ§Ø¨Ø¹Ø©',
        r'Ù…ØªØ§Ø¨Ø¹Ø©',
        r'ØªÙ… Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­ØŒ Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ!',
        r'Ø¥ØºÙ„Ø§Ù‚'
    ]
    
    for pattern in unwanted_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    lines = text.split('\n')
    filtered_lines = []
    
    for line in lines:
        line = line.strip()
        if line:
            if re.match(r'^[\d\s\-\.\*\(\)]+$', line):
                continue
            
            if len(line) < 20:
                continue
            
            if re.search(r'(.{2,})\1{3,}', line):
                continue
            
            filtered_lines.append(line)
    
    return '\n\n'.join(filtered_lines)

def clean_and_organize_text(text: str) -> str:
    """ØªÙ†Ø¸ÙŠÙ ÙˆØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
    if not text:
        return ""
    
    text = text.strip()
    
    while "\n\n\n" in text:
        text = text.replace("\n\n\n", "\n\n")
    
    text = re.sub(r'^\n+', '', text)
    text = re.sub(r'\n+$', '', text)
    text = re.sub(r' +', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    text = filter_content_quality(text)
    text = remove_duplicate_content(text)
    
    return text.strip()

def search_google(query: str, num_results: int = 5) -> list:
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Google Ø¹Ù† Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹"""
    try:
        urls = []
        for url in search(query, num_results=num_results, lang="ar", country="sa"):
            urls.append(url)
        return urls
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Google: {e}")
        return []

def extract_text_from_url(url: str, min_length: int = 100) -> dict:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ ÙÙ‚Ø· Ù…Ù† Ø±Ø§Ø¨Ø· ÙˆØ§Ø­Ø¯ Ù…Ø¹ Ø§Ù„ØªÙ†Ø¸ÙŠÙ… ÙˆØ§Ù„ÙÙ„ØªØ±Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ar,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        unwanted_elements = [
            'nav', 'header', 'footer', 'aside', 'menu',
            'script', 'style', 'noscript', 'iframe',
            '.navigation', '.menu', '.sidebar', '.footer',
            '.header', '.ads', '.advertisement', '.social',
            '.comments', '.related', '.recommended',
            '.breadcrumb', '.pagination', '.search',
            '.newsletter', '.subscribe', '.newsletter-signup'
        ]
        
        for element in unwanted_elements:
            if element.startswith('.'):
                for unwanted in soup.select(element):
                    unwanted.decompose()
            else:
                for unwanted in soup.find_all(element):
                    unwanted.decompose()
        
        main_content = None
        selectors = [
            'article', 'main', 
            '[role="main"]', 
            '.post-content', '.article-content', '.entry-content',
            '#content', '#main-content', '#article-content',
            '.content', '.main-content', '.article-body',
            '.post-body', '.entry-body', '.article-text'
        ]
        
        for selector in selectors:
            main_content = soup.select_one(selector)
            if main_content:
                break
        
        search_area = main_content if main_content else soup
        
        text_elements = search_area.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div'], recursive=True)
        
        article_texts = []
        
        for element in text_elements:
            text = element.get_text(strip=True)
            
            if len(text) > 30 and len(text) < 2000:
                ui_keywords = [
                    'click here', 'read more', 'subscribe', 'follow us', 'share', 'like',
                    'comment', 'download', 'upload', 'login', 'register', 'sign up',
                    'ØªÙ…Øª Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø¨ÙˆØ§Ø³Ø·Ø©', 'Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«', 'Ù…Ø­ØªÙˆÙŠØ§Øª', 'Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹',
                    'Ù‡Ù„ Ø§Ø³ØªÙØ¯Øª', 'Ù‡Ù„ ÙƒØ§Ù† Ø§Ù„Ù…Ù‚Ø§Ù„ Ù…ÙÙŠØ¯Ø§Ù‹', 'Ù†Ø¹Ù…', 'Ù„Ø§',
                    'Ø£Ø¶Ù Ø§Ù„Ø³Ø¨Ø¨', 'Ù‡Ø°Ø§ Ø§Ù„Ø­Ù‚Ù„ Ù…Ø·Ù„ÙˆØ¨', 'Ø§Ù„Ø±Ø¬Ø§Ø¡ ÙƒØªØ§Ø¨Ø© Ù†Øµ ØµØ­ÙŠØ­',
                    'Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚', 'Ø¥Ù„ØºØ§Ø¡', 'Ø´Ø§Ø±Ùƒ Ø§Ù„Ù…Ù‚Ø§Ù„Ø©', 'ÙÙŠØ³Ø¨ÙˆÙƒ', 'ØªÙˆÙŠØªØ±',
                    'Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø°Ø§Øª ØµÙ„Ø©', 'Ø§Ù†Ø¶Ù… Ø§Ù„ÙŠÙ†Ø§', 'ØªÙˆØ§ØµÙ„ Ù…Ø¹Ù†Ø§', 'Tags:',
                    'Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø§Øª', 'Ø§Ø¹Ø±Ù Ø§Ù„Ù…Ø²ÙŠØ¯', 'Ø¯ÙˆØ±Ø§Øª Ù‚Ø¯ ØªÙ‡Ù…Ùƒ', 'Ù…Ø³Ø§Ø±Ø§Øª ØªØ¹Ù„ÙŠÙ…ÙŠØ©',
                    'ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª', 'Ù…Ù‚Ø§Ù„Ø§Øª Ø´Ø§Ø¦Ø¹Ø©', 'ÙƒØªØ§Ø¨Ø© :', 'Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«:',
                    'ØªØ­Ù…ÙŠÙ„', 'Ù…Ø´Ø§Ù‡Ø¯Ø©', 'Ù„Ù„Ù…Ø²ÙŠØ¯', 'ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰'
                ]
                
                if any(keyword in text.lower() for keyword in ui_keywords):
                    continue
                
                if re.match(r'^[\d\s\-\.]+$', text):
                    continue
                
                if re.search(r'(.{3,})\1{2,}', text):
                    continue
                
                if any(char in text for char in ['http://', 'https://', 'www.', '.com', '.org']):
                    continue
                
                if re.search(r'[<>{}[\]]', text):
                    continue
                
                article_texts.append(text)
        
        if article_texts:
            raw_text = "\n\n".join(article_texts)
            clean_text = clean_and_organize_text(raw_text)
            
            if clean_text and len(clean_text) > min_length:
                return {
                    "status": "success",
                    "url": url,
                    "paragraphs_count": len(article_texts),
                    "text": clean_text,
                    "text_length": len(clean_text)
                }
        
        all_text = search_area.get_text()
        if all_text:
            clean_text = clean_and_organize_text(all_text)
            if len(clean_text) > min_length:
                return {
                    "status": "success",
                    "url": url,
                    "paragraphs_count": 1,
                    "text": clean_text,
                    "text_length": len(clean_text),
                    "note": "ØªÙ… Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¨Ø¯ÙŠÙ„Ø©"
                }
        
        raise ValueError("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ù†ØµÙŠ ÙˆØ§Ø¶Ø­ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„ØµÙØ­Ø©")
        
    except requests.exceptions.RequestException as e:
        return {
            "status": "error",
            "url": url,
            "error": f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©: {e}"
        }
    except Exception as e:
        return {
            "status": "error",
            "url": url,
            "error": f"Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}"
        }

@app.route('/search-articles/', methods=['POST'])
def search_articles():
    """Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù†Ù‡Ø§"""
    try:
        data = request.get_json()
        query = data.get('query', '')
        min_length = data.get('min_length', 100)
        num_results = data.get('num_results', 5)  # Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        
        if not query:
            return jsonify({"error": "ÙŠØ¬Ø¨ ØªÙˆÙÙŠØ± query"}), 400
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Google Ø¹Ù† Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹
        print(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Google Ø¹Ù†: {query}")
        print(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: {num_results}")
        
        urls = search_google(query, num_results)
        
        if not urls:
            return jsonify({
                "success": False,
                "error": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Google",
                "query": query
            }), 404
        
        print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(urls)} Ù…ÙˆÙ‚Ø¹")
        
        results = []
        successful_articles = 0
        failed_articles = 0
        
        for url in urls:
            print(f"ğŸ“ Ù…Ø¹Ø§Ù„Ø¬Ø©: {url}")
            result = extract_text_from_url(url, min_length)
            if result["status"] == "success":
                successful_articles += 1
                results.append(result)
            else:
                failed_articles += 1
                results.append(result)
        
        response_data = {
            "success": True,
            "query": query,
            "total_urls": len(urls),
            "successful_articles": successful_articles,
            "failed_articles": failed_articles,
            "results": results,
            "min_length_used": min_length,
            "num_results_requested": num_results,
            "search_engine": "Google"
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        return jsonify({"error": f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨: {str(e)}"}), 500

@app.route('/search-google/', methods=['POST'])
def search_google_only():
    """Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Ù„Ù„Ø¨Ø­Ø« ÙÙŠ Google ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ)"""
    try:
        data = request.get_json()
        query = data.get('query', '')
        num_results = data.get('num_results', 5)
        
        if not query:
            return jsonify({"error": "ÙŠØ¬Ø¨ ØªÙˆÙÙŠØ± query"}), 400
        
        print(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Google Ø¹Ù†: {query}")
        print(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: {num_results}")
        
        urls = search_google(query, num_results)
        
        if not urls:
            return jsonify({
                "success": False,
                "error": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Google",
                "query": query
            }), 404
        
        response_data = {
            "success": True,
            "query": query,
            "total_results": len(urls),
            "num_results_requested": num_results,
            "search_engine": "Google",
            "urls": urls
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        return jsonify({"error": f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨: {str(e)}"}), 500

@app.route('/extract-single/', methods=['POST'])
def extract_single_article():
    """Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ Ù…Ù† Ø±Ø§Ø¨Ø· ÙˆØ§Ø­Ø¯"""
    try:
        data = request.get_json()
        url = data.get('url', '')
        min_length = data.get('min_length', 100)
        
        if not url:
            return jsonify({"error": "ÙŠØ¬Ø¨ ØªÙˆÙÙŠØ± url"}), 400
        
        result = extract_text_from_url(url, min_length)
        return jsonify(result)
        
    except Exception as e:
        return jsonify({"error": f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨: {str(e)}"}), 500

@app.route('/health', methods=['GET'])
def health_check():
    """ÙØ­Øµ ØµØ­Ø© API"""
    return jsonify({
        "status": "healthy",
        "message": "Article Extractor API is running",
        "features": [
            "Improved text extraction",
            "Duplicate content removal",
            "Content quality filtering",
            "Customizable minimum text length",
            "Google search integration",
            "Configurable number of results"
        ],
        "endpoints": [
            "POST /search-articles/ - Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Google ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ",
            "POST /search-google/ - Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Google ÙÙ‚Ø·",
            "POST /extract-single/ - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ Ù…Ù† Ø±Ø§Ø¨Ø· ÙˆØ§Ø­Ø¯",
            "GET /health - ÙØ­Øµ ØµØ­Ø© API"
        ]
    })

# Export the Flask app for Vercel
if __name__ == "__main__":
    app.run()
