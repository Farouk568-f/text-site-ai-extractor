# -*- coding: utf-8 -*-
"""
API ูุญุณู ูุงุณุชุฎุฑุงุฌ ุงููุต ูู ุงูููุงูุงุช ุงูุนุฑุจูุฉ
ูุญุชูู ุนูู ุฏูุงู ูุญุณูุฉ ูุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ูุงููุญุชูู ุบูุฑ ุงููุฑุบูุจ
"""

import re
import difflib
import requests
from bs4 import BeautifulSoup
from flask import Flask, request, jsonify
from flask_cors import CORS
import time
import uuid

app = Flask(__name__)
CORS(app)

# ุชุฎุฒูู ุงูููุงู
tasks = {}

def remove_duplicate_content(text: str) -> str:
    """
    ุฅุฒุงูุฉ ุงููุญุชูู ุงูููุฑุฑ ูุงููุชูุฑุฑ ูู ุงููุต
    """
    if not text:
        return ""
    
    # ุชูุณูู ุงููุต ุฅูู ููุฑุงุช
    paragraphs = text.split('\n\n')
    
    # ุฅุฒุงูุฉ ุงูููุฑุงุช ุงููุงุฑุบุฉ
    paragraphs = [p.strip() for p in paragraphs if p.strip()]
    
    # ุฅุฒุงูุฉ ุงูููุฑุงุช ุงููุชูุฑุฑุฉ ุจุงุณุชุฎุฏุงู ุฎูุงุฑุฒููุฉ ุฐููุฉ
    unique_paragraphs = []
    seen_paragraphs = set()
    
    for paragraph in paragraphs:
        if len(paragraph) < 50:  # ุชุฌุงูู ุงูููุฑุงุช ุงููุตูุฑุฉ ุฌุฏุงู
            continue
            
        # ุชูุธูู ุงูููุฑุฉ
        clean_paragraph = re.sub(r'\s+', ' ', paragraph.strip())
        
        # ุญุณุงุจ ุงูุชุดุงุจู ูุน ุงูููุฑุงุช ุงูุณุงุจูุฉ
        is_duplicate = False
        for seen in seen_paragraphs:
            # ุงุณุชุฎุฏุงู ุฎูุงุฑุฒููุฉ ุชุดุงุจู ุงููุต
            similarity = difflib.SequenceMatcher(None, clean_paragraph, seen).ratio()
            if similarity > 0.7:  # ุฅุฐุง ูุงู ุงูุชุดุงุจู ุฃูุซุฑ ูู 70%
                is_duplicate = True
                break
        
        if not is_duplicate:
            unique_paragraphs.append(paragraph)
            seen_paragraphs.add(clean_paragraph)
    
    return '\n\n'.join(unique_paragraphs)

def filter_content_quality(text: str) -> str:
    """
    ููุชุฑุฉ ุฌูุฏุฉ ุงููุญุชูู ูุฅุฒุงูุฉ ุงููุตูุต ุบูุฑ ุงููุฑุบูุจุฉ
    """
    if not text:
        return ""
    
    # ูุงุฆูุฉ ุงููููุงุช ุงูููุชุงุญูุฉ ุงูุชู ุชุดูุฑ ุฅูู ูุญุชูู ุบูุฑ ูุฑุบูุจ
    unwanted_patterns = [
        r'ุงูุฐูุงุก ุงูุงุตุทูุงุนูSDAIAุงูููุฆุฉ ุงูุณุนูุฏูุฉ ููุจูุงูุงุช ูุงูุฐูุงุก ุงูุงุตุทูุงุนููุนูููุงุช ุณุฏุงูุงุนู ุณุฏุงูุงุงูุฐูุงุก ุงูุงุตุทูุงุนู',
        r'โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ',
        r'ูู ุงุณุชูุฏุช ูู ุงููุนูููุงุช ุงูููุฏูุฉ ูู ูุฐู ุงูุตูุญุฉุ',
        r'ูุนููุง0ูู ุงูุฒูุงุฑ ุฃุนุฌุจูู ูุญุชูู ุงูุตูุญุฉ ูู ุฃุตู0',
        r'ุดูุฑุง ููุดุงุฑูุชู ุชู ุงุณุชูุงู ููุงุญุธุงุชู ุจูุฌุงุญ',
        r'ููุฏ ููุช ุณุงุจูุง ุจุชูุฏูู ููุงุญุธุงุชู',
        r'ุฃุถู ุงูุณุจุจ \(ุงุฎุชุฑ ูู ุฎูุงุฑ ูุงุญุฏ ุฅูู ุฎูุงุฑุงู\)',
        r'ูุฐุง ุงูุญูู ูุทููุจ',
        r'ุงูุฑุฌุงุก ูุชุงุจุฉ ูุต ุตุญูุญ',
        r'ุงูููุงุญุธุงุช ูุฌุจ ุงู ุชููู ุงูู ูู ูขูฅู ุญุฑู',
        r'ุฅุฑุณุงู ุงูุชุนููู',
        r'ุฅูุบุงุก',
        r'ุดุงุฑู ุงูููุงูุฉ',
        r'ููุณุจููุชููุชุฑ',
        r'ููุงุถูุน ุฐุงุช ุตูุฉ ุจู :',
        r'ุงูุถู ุงูููุง ุงูุงู',
        r'ูู ุฌุฒุก ูู ุบูุงุฆู ู ุทูุฑ ูู ูุชุฌุฑู',
        r'ุงูุธู ุงูููุง ูุงู',
        r'Tags:',
        r'ุงูููุชุฑุญุงุช',
        r'ุงุนุฑู ุงููุฒูุฏ',
        r'ุฏูุฑุงุช ูุฏ ุชููู',
        r'ูุณุงุฑุงุช ุชุนููููุฉ',
        r'ููุฑุณ ุงููุญุชููุงุช',
        r'ููุงูุงุช ุดุงุฆุนุฉ',
        r'ูุชุงุจุฉ :',
        r'ุขุฎุฑ ุชุญุฏูุซ:',
        r'ุชุญููู',
        r'ูุดุงูุฏุฉ',
        r'ูููุฒูุฏ ูุฑุฌู ุงูุงุทูุงุน ุนูู',
        r'ุชูุช ุงููุชุงุจุฉ ุจูุงุณุทุฉ:',
        r'ุขุฎุฑ ุชุญุฏูุซ:',
        r'ูุญุชููุงุช',
        r'ุงููุฑุงุฌุน',
        r'ูุฌููุจุฉ ูู',
        r'ูู ูุงู ุงูููุงู ูููุฏุงูุ',
        r'ุงูุฑุฌุงุก ุฅุฌุชูุงุฒ ุงูุฅุฎุชุจุงุฑ',
        r'ุฑุงุฆุน!',
        r'ูู ูุฏูู ููุชุฑุญุงุช ูุชุญุณูู ุชุฌุฑุจุชู ุฃูุซุฑุ',
        r'ุจุฑูุฏู ุงูุฅููุชุฑููู\*',
        r'ูุฐุง ุงูุญูู ูุทููุจ',
        r'ุงูุฅุณู\*',
        r'ุงูุฅุดุชุฑุงู ุจูุงุฆูุฉ ุจุฑูุฏ ููุถูุน',
        r'ูุฃุณู ูุฐูู!',
        r'ููุงุฐุง ูุงู ุงูููุงู ุบูุฑ ูููุฏุ',
        r'ูุทููุจ ุงูุฅุฎุชูุงุฑ',
        r'ูุฌุจ ุงูุงุฎุชูุงุฑ ูููุชุงุจุนุฉ',
        r'ูุชุงุจุนุฉ',
        r'ุชู ุงูุฅุฑุณุงู ุจูุฌุงุญุ ุดูุฑุงู ูู!',
        r'ุฅุบูุงู',
        r'ุงูุถู ุงูููุง ุงูุงู',
        r'ูู ุฌุฒุก ูู ุบูุงุฆู ู ุทูุฑ ูู ูุชุฌุฑู ู ุชููู ุงูุชูุจ ุชุฑูุฏ ูู ุงูููููุฉ',
        r'ุงูุธู ุงูููุง ูุงู'
    ]
    
    # ุฅุฒุงูุฉ ุงูุฃููุงุท ุบูุฑ ุงููุฑุบูุจุฉ
    for pattern in unwanted_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # ุฅุฒุงูุฉ ุงูุฃุณุทุฑ ุงูุชู ุชุญุชูู ุนูู ุฑููุฒ ุฃู ุฃุฑูุงู ููุท
    lines = text.split('\n')
    filtered_lines = []
    
    for line in lines:
        line = line.strip()
        if line:
            # ุชุฌุงูู ุงูุฃุณุทุฑ ุงูุชู ุชุญุชูู ุนูู ุฑููุฒ ุฃู ุฃุฑูุงู ููุท
            if re.match(r'^[\d\s\-\.\*\(\)]+$', line):
                continue
            
            # ุชุฌุงูู ุงูุฃุณุทุฑ ุงููุตูุฑุฉ ุฌุฏุงู
            if len(line) < 20:
                continue
            
            # ุชุฌุงูู ุงูุฃุณุทุฑ ุงูุชู ุชุญุชูู ุนูู ุชูุฑุงุฑุงุช ูุงุถุญุฉ
            if re.search(r'(.{2,})\1{3,}', line):
                continue
            
            filtered_lines.append(line)
    
    return '\n\n'.join(filtered_lines)

def clean_and_organize_text(text: str) -> str:
    """
    ุชูุธูู ูุชูุธูู ุงููุต ุงููุณุชุฎุฑุฌ ูู ุงููููุน ูุน ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ูุงููุญุชูู ุบูุฑ ุงููุฑุบูุจ
    """
    if not text:
        return ""
    
    # 1. ุฅุฒุงูุฉ ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ ูู ุงูุจุฏุงูุฉ ูุงูููุงูุฉ
    text = text.strip()
    
    # 2. ุชูุธูู ุงูููุงุตู ุงููุชูุฑุฑุฉ ุจูู ุงูููุฑุงุช
    while "\n\n\n" in text:
        text = text.replace("\n\n\n", "\n\n")
    
    # 3. ุฅุฒุงูุฉ ุงูุฃุณุทุฑ ุงููุงุฑุบุฉ ูู ุงูุจุฏุงูุฉ ูุงูููุงูุฉ
    text = re.sub(r'^\n+', '', text)
    text = re.sub(r'\n+$', '', text)
    
    # 4. ุชูุธูู ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ ุฏุงุฎู ุงููุต
    text = re.sub(r' +', ' ', text)
    
    # 5. ุชูุธูู ุงูููุงุตู ุงููุชูุฑุฑุฉ
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # 6. ููุชุฑุฉ ุฌูุฏุฉ ุงููุญุชูู
    text = filter_content_quality(text)
    
    # 7. ุฅุฒุงูุฉ ุงููุญุชูู ุงูููุฑุฑ
    text = remove_duplicate_content(text)
    
    # 8. ุชูุธูู ููุงุฆู
    text = text.strip()
    
    return text

def extract_text_from_url(url: str, min_length: int = 100) -> dict:
    """
    ุงุณุชุฎุฑุงุฌ ุงููุต ููุท ูู ุฑุงุจุท ูุงุญุฏ ูุน ุงูุชูุธูู ูุงูููุชุฑุฉ ุงููุชูุฏูุฉ
    """
    print(f"๐ ุฌุงุฑู ุงุณุชุฎุฑุงุฌ ุงููุต ูู: {url}")
    
    try:
        # ุฅุนุฏุงุฏ headers ููุญุงูุงุฉ ุงููุชุตูุญ
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ar,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        
        # ุฌูุจ ุงูุตูุญุฉ
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        
        # ุชุญููู HTML
        soup = BeautifulSoup(response.text, "html.parser")
        
        # โญ ุฅุฒุงูุฉ ุงูุนูุงุตุฑ ุบูุฑ ุงููุฑุบูุจุฉ ุฃููุงู โญ
        unwanted_elements = [
            'nav', 'header', 'footer', 'aside', 'menu',
            'script', 'style', 'noscript', 'iframe',
            '.navigation', '.menu', '.sidebar', '.footer',
            '.header', '.ads', '.advertisement', '.social',
            '.comments', '.related', '.recommended',
            '.breadcrumb', '.pagination', '.search',
            '.newsletter', '.subscribe', '.newsletter-signup'
        ]
        
        for element in unwanted_elements:
            if element.startswith('.'):
                # ุฅุฒุงูุฉ ุงูุนูุงุตุฑ ุจุงูู class
                for unwanted in soup.select(element):
                    unwanted.decompose()
            else:
                # ุฅุฒุงูุฉ ุงูุนูุงุตุฑ ุจุงูู tag
                for unwanted in soup.find_all(element):
                    unwanted.decompose()
        
        # โญ ุงูุจุญุซ ุนู ุงููุญุชูู ุงูุฑุฆูุณู ุจุฐูุงุก โญ
        main_content = None
        
        # ุงูุจุญุซ ูู ุชุฑุชูุจ ุงูุฃููููุฉ
        selectors = [
            'article', 'main', 
            '[role="main"]', 
            '.post-content', '.article-content', '.entry-content',
            '#content', '#main-content', '#article-content',
            '.content', '.main-content', '.article-body',
            '.post-body', '.entry-body', '.article-text'
        ]
        
        for selector in selectors:
            main_content = soup.select_one(selector)
            if main_content:
                break
        
        # ุฅุฐุง ูู ูุฌุฏ ูุญุชูู ุฑุฆูุณูุ ูุจุญุซ ูู ุงูุตูุญุฉ ูููุง
        search_area = main_content if main_content else soup
        
        # โญ ุงุณุชุฎุฑุงุฌ ุงููุต ุจุทุฑููุฉ ุฐููุฉ ููุชูุฏูุฉ โญ
        # 1. ุงูุจุญุซ ุนู ุงูููุฑุงุช ูุงูุนูุงููู
        text_elements = search_area.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'div'], recursive=True)
        
        # 2. ุชุฌููุน ุงููุต ูุน ููุชุฑุฉ ูุชูุฏูุฉ
        article_texts = []
        
        for element in text_elements:
            # ุงูุญุตูู ุนูู ุงููุต
            text = element.get_text(strip=True)
            
            # ููุชุฑุฉ ุงููุต ุงููุชูุฏูุฉ
            if len(text) > 30 and len(text) < 2000:
                # ุฅุฒุงูุฉ ุงููุตูุต ุงูุชู ุชุญุชูู ุนูู ุนูุงุตุฑ ูุงุฌูุฉ ุงููุณุชุฎุฏู
                ui_keywords = [
                    'click here', 'read more', 'subscribe', 'follow us', 'share', 'like',
                    'comment', 'download', 'upload', 'login', 'register', 'sign up',
                    'ุชูุช ุงููุชุงุจุฉ ุจูุงุณุทุฉ', 'ุขุฎุฑ ุชุญุฏูุซ', 'ูุญุชููุงุช', 'ุงููุฑุงุฌุน',
                    'ูู ุงุณุชูุฏุช', 'ูู ูุงู ุงูููุงู ูููุฏุงู', 'ูุนู', 'ูุง',
                    'ุฃุถู ุงูุณุจุจ', 'ูุฐุง ุงูุญูู ูุทููุจ', 'ุงูุฑุฌุงุก ูุชุงุจุฉ ูุต ุตุญูุญ',
                    'ุฅุฑุณุงู ุงูุชุนููู', 'ุฅูุบุงุก', 'ุดุงุฑู ุงูููุงูุฉ', 'ููุณุจูู', 'ุชููุชุฑ',
                    'ููุงุถูุน ุฐุงุช ุตูุฉ', 'ุงูุถู ุงูููุง', 'ุชูุงุตู ูุนูุง', 'Tags:',
                    'ุงูููุชุฑุญุงุช', 'ุงุนุฑู ุงููุฒูุฏ', 'ุฏูุฑุงุช ูุฏ ุชููู', 'ูุณุงุฑุงุช ุชุนููููุฉ',
                    'ููุฑุณ ุงููุญุชููุงุช', 'ููุงูุงุช ุดุงุฆุนุฉ', 'ูุชุงุจุฉ :', 'ุขุฎุฑ ุชุญุฏูุซ:',
                    'ุชุญููู', 'ูุดุงูุฏุฉ', 'ูููุฒูุฏ', 'ูุฑุฌู ุงูุงุทูุงุน ุนูู'
                ]
                
                if any(keyword in text.lower() for keyword in ui_keywords):
                    continue
                
                # ุฅุฒุงูุฉ ุงููุตูุต ุงูุชู ุชุญุชูู ุนูู ุฃุฑูุงู ุฃู ุฑููุฒ ูุชูุฑุฑุฉ
                if re.match(r'^[\d\s\-\.]+$', text):
                    continue
                
                # ุฅุฒุงูุฉ ุงููุตูุต ุงูุชู ุชุญุชูู ุนูู ุชูุฑุงุฑุงุช ูุงุถุญุฉ
                if re.search(r'(.{3,})\1{2,}', text):
                    continue
                
                # ุฅุฒุงูุฉ ุงููุตูุต ุงูุชู ุชุญุชูู ุนูู ุฑูุงุจุท ุฃู ุฃุฒุฑุงุฑ
                if any(char in text for char in ['http://', 'https://', 'www.', '.com', '.org']):
                    continue
                
                # ุฅุฒุงูุฉ ุงููุตูุต ุงูุชู ุชุญุชูู ุนูู ุฑููุฒ HTML ุฃู CSS
                if re.search(r'[<>{}[\]]', text):
                    continue
                
                article_texts.append(text)
        
        # 3. ุชุฌููุน ุงููุต ูุน ููุงุตู ูุงุถุญุฉ
        if article_texts:
            raw_text = "\n\n".join(article_texts)
            # ุชูุธูู ูุชูุธูู ุงููุต
            clean_text = clean_and_organize_text(raw_text)
            
            if clean_text and len(clean_text) > min_length:  # ุงุณุชุฎุฏุงู ุงูุญุฏ ุงูุฃุฏูู ุงููุญุฏุฏ
                print(f"โ ุชู ุงุณุชุฎุฑุงุฌ {len(article_texts)} ููุฑุฉ ุจูุฌุงุญ")
                return {
                    "status": "success",
                    "url": url,
                    "paragraphs_count": len(article_texts),
                    "text": clean_text,
                    "text_length": len(clean_text)
                }
        
        # ุฅุฐุง ูู ูุฌุฏ ูุญุชูู ูู ุงูููุฑุงุชุ ูุฌุฑุจ ุทุฑููุฉ ุจุฏููุฉ
        print("โ๏ธ  ูู ูุฌุฏ ูุญุชูู ูู ุงูููุฑุงุชุ ูุฌุฑุจ ุทุฑููุฉ ุจุฏููุฉ...")
        
        # ุงูุจุญุซ ูู ุฌููุน ุงููุตูุต ูุน ููุชุฑุฉ ุฅุถุงููุฉ
        all_text = search_area.get_text()
        if all_text:
            # ุชูุธูู ุงููุต
            clean_text = clean_and_organize_text(all_text)
            if len(clean_text) > min_length:  # ุงุณุชุฎุฏุงู ุงูุญุฏ ุงูุฃุฏูู ุงููุญุฏุฏ
                return {
                    "status": "success",
                    "url": url,
                    "paragraphs_count": 1,
                    "text": clean_text,
                    "text_length": len(clean_text),
                    "note": "ุชู ุงูุงุณุชุฎุฑุงุฌ ุจุทุฑููุฉ ุจุฏููุฉ"
                }
        
        raise ValueError("ูู ูุชู ุงูุนุซูุฑ ุนูู ูุญุชูู ูุตู ูุงุถุญ ูู ูุฐู ุงูุตูุญุฉ")
        
    except requests.exceptions.RequestException as e:
        print(f"โ ุฎุทุฃ ูู ุฌูุจ ุงูุตูุญุฉ: {e}")
        return {
            "status": "error",
            "url": url,
            "error": f"ุฎุทุฃ ูู ุฌูุจ ุงูุตูุญุฉ: {e}"
        }
    except Exception as e:
        print(f"โ ุฎุทุฃ ุบูุฑ ูุชููุน: {e}")
        return {
            "status": "error",
            "url": url,
            "error": f"ุฎุทุฃ ุบูุฑ ูุชููุน: {e}"
        }

@app.route('/search-articles/', methods=['POST'])
def search_articles():
    """
    ููุทุฉ ููุงูุฉ ููุจุญุซ ุนู ุงูููุงูุงุช ูุงุณุชุฎุฑุงุฌ ุงููุต ูููุง
    """
    try:
        data = request.get_json()
        query = data.get('query', '')
        min_length = data.get('min_length', 100)  # ุงูุญุฏ ุงูุฃุฏูู ูุทูู ุงููุต (ูุงุจู ููุชุฎุตูุต)
        
        if not query:
            return jsonify({"error": "ูุฌุจ ุชูููุฑ query"}), 400
        
        print(f"๐ ุงูุจุญุซ ุนู: {query}")
        print(f"๐ ุงูุญุฏ ุงูุฃุฏูู ูุทูู ุงููุต: {min_length}")
        
        # ููุง ููููู ุฅุถุงูุฉ ููุทู ุงูุจุญุซ ุนู ุงูุฑูุงุจุท
        # ูู ุงูููุช ุงูุญุงููุ ุณูุณุชุฎุฏู ูุซุงู ุจุณูุท
        sample_urls = [
            "https://sdaia.gov.sa/ar/SDAIA/about/Pages/AboutAI.aspx",
            "https://mawdoo3.com/%D8%A3%D9%87%D9%85%D9%8A%D8%A9_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A"
        ]
        
        results = []
        successful_articles = 0
        failed_articles = 0
        
        for url in sample_urls:
            result = extract_text_from_url(url, min_length)
            if result["status"] == "success":
                successful_articles += 1
                results.append(result)
            else:
                failed_articles += 1
                results.append(result)
        
        response_data = {
            "success": True,
            "query": query,
            "total_urls": len(sample_urls),
            "successful_articles": successful_articles,
            "failed_articles": failed_articles,
            "results": results,
            "min_length_used": min_length
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        return jsonify({"error": f"ุฎุทุฃ ูู ูุนุงูุฌุฉ ุงูุทูุจ: {str(e)}"}), 500

@app.route('/extract-single/', methods=['POST'])
def extract_single_article():
    """
    ููุทุฉ ููุงูุฉ ูุงุณุชุฎุฑุงุฌ ูุต ูู ุฑุงุจุท ูุงุญุฏ
    """
    try:
        data = request.get_json()
        url = data.get('url', '')
        min_length = data.get('min_length', 100)  # ุงูุญุฏ ุงูุฃุฏูู ูุทูู ุงููุต
        
        if not url:
            return jsonify({"error": "ูุฌุจ ุชูููุฑ url"}), 400
        
        print(f"๐ ุงุณุชุฎุฑุงุฌ ุงููุต ูู: {url}")
        print(f"๐ ุงูุญุฏ ุงูุฃุฏูู ูุทูู ุงููุต: {min_length}")
        
        result = extract_text_from_url(url, min_length)
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({"error": f"ุฎุทุฃ ูู ูุนุงูุฌุฉ ุงูุทูุจ: {str(e)}"}), 500

@app.route('/health', methods=['GET'])
def health_check():
    """
    ูุญุต ุตุญุฉ API
    """
    return jsonify({
        "status": "healthy",
        "message": "Article Extractor API is running",
        "features": [
            "Improved text extraction",
            "Duplicate content removal",
            "Content quality filtering",
            "Customizable minimum text length"
        ]
    })

if __name__ == "__main__":
    print("๐ ุชุดุบูู Article Extractor API...")
    print("๐ ุงูููุงุท ุงูููุงุฆูุฉ ุงููุชุงุญุฉ:")
    print("   POST /search-articles/ - ุงูุจุญุซ ุนู ุงูููุงูุงุช")
    print("   POST /extract-single/ - ุงุณุชุฎุฑุงุฌ ูุต ูู ุฑุงุจุท ูุงุญุฏ")
    print("   GET  /health - ูุญุต ุตุญุฉ API")
    print("\n๐ก ููููู ุชุฎุตูุต ุงูุญุฏ ุงูุฃุฏูู ูุทูู ุงููุต ุจุงุณุชุฎุฏุงู min_length")
    print("   ูุซุงู: {'query': 'ุฃูููุฉ ุงูุฐูุงุก ุงูุงุตุทูุงุนู', 'min_length': 1000}")
    
    app.run(host='0.0.0.0', port=5001, debug=True)
